{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded a dataset of apple quality on kaggle : https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality\n",
    "\n",
    "The goal is to predict if an apple is good or bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv(os.path.join(\"apple_quality\", \"apple_quality.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I can see that the acidity is also an object whereas it's composed of numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple[\"Acidity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.hist(bins = 50, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = apple.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I delete the only value that contains null\n",
    "apple_cleaned = apple.dropna()\n",
    "\n",
    "# So now only one line has been deleted\n",
    "print(\"Number of line :\", len(apple_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_cleaned[\"Acidity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I convert the acidity\n",
    "apple_cleaned['Acidity'] = pd.to_numeric(apple['Acidity'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_cleaned.info() #Now acidities are float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_cleaned[\"Quality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(apple_cleaned, apple_cleaned[\"Quality\"]):\n",
    "    train_set = apple_cleaned.loc[train_index]\n",
    "    test_set = apple_cleaned.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"Quality\"].value_counts() / len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"Quality\"].value_counts() / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_cleaned[\"Quality\"].value_counts() / len(apple_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repartition is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_cat_proportions(data):\n",
    "    return data[\"Quality\"].value_counts() / len(data)\n",
    "\n",
    "compare_props = pd.DataFrame({\n",
    "    \"Overall\": income_cat_proportions(apple_cleaned),\n",
    "    \"Stratified\": income_cat_proportions(train_set),\n",
    "    \"Random\": income_cat_proportions(train_set),\n",
    "}).sort_index()\n",
    "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
    "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's discover our datas !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer un dictionnaire de mapping\n",
    "quality_mapping = {'good': 1, 'bad': 0}\n",
    "\n",
    "# Appliquer le mapping à la colonne Quality\n",
    "apple_train['Quality'] = apple_train['Quality'].map(quality_mapping)\n",
    "\n",
    "# Maintenant, vous pouvez calculer la matrice de corrélation\n",
    "correlation_matrix = apple_train.corr()\n",
    "\n",
    "# Tracer la heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Matrice de corrélation')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix[\"Quality\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see which features can help us to predict the quality of an apple. We could have guessed it but the juice, the sweetness, the size and the ripeness are correlated. In my mind, acidity and crunchiness would have played a bigger role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"Quality\", \"Juiciness\", \"Sweetness\",\n",
    "              \"Size\", \"Ripeness\" ]\n",
    "scatter_matrix(apple_train[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Création de la figure\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Liste des combinaisons de variables\n",
    "combinations = [(\"Ripeness\", \"Juiciness\", \"Sweetness\"),\n",
    "                (\"Quality\", \"Size\", \"Ripeness\")]\n",
    "\n",
    "# Boucle sur les combinaisons pour créer les graphiques 3D\n",
    "for i, combo in enumerate(combinations, start=1):\n",
    "    ax = fig.add_subplot(2, 2, i, projection='3d')\n",
    "    ax.scatter(apple_train[combo[0]], apple_train[combo[1]], apple_train[combo[2]])\n",
    "    ax.set_xlabel(combo[0])\n",
    "    ax.set_ylabel(combo[1])\n",
    "    ax.set_zlabel(combo[2])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test attribute combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_train[\"Ratio Juice Sweet\"] = apple_train[\"Juiciness\"]/apple_train[\"Sweetness\"]\n",
    "apple_train[\"Ratio Juice size\"] = apple_train[\"Juiciness\"]/apple_train[\"Size\"]\n",
    "apple_train[\"Ratio Juice Ripeness\"] = apple_train[\"Juiciness\"]/apple_train[\"Ripeness\"]\n",
    "apple_train[\"Ratio Sweet Size\"] = apple_train[\"Sweetness\"]/apple_train[\"Size\"]\n",
    "\n",
    "apple_train[\"Ratio Sweet Ripeness\"] = apple_train[\"Sweetness\"]/apple_train[\"Ripeness\"]\n",
    "\n",
    "apple_train[\"Ratio Size Ripeness\"] = apple_train[\"Size\"]/apple_train[\"Ripeness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = apple_train.corr()\n",
    "corr_matrix[\"Quality\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to create attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's delete them\n",
    "attributes_to_delete = [\"Ratio Juice Sweet\", \"Ratio Juice size\", \"Ratio Juice Ripeness\",\n",
    "                         \"Ratio Sweet Size\", \"Ratio Sweet Ripeness\", \"Ratio Size Ripeness\"]\n",
    "\n",
    "apple_train.drop(attributes_to_delete, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "apple = apple_train.drop(\"Quality\", axis=1) #Drop the label\n",
    "apple_label = apple_train[\"Quality\"].copy() #Save the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to clean the data as it's from kaggle, I've already removed one apple that had missing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows = apple[apple.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_features = apple.select_dtypes(include=['float64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "apple_prepared = preprocessor.fit_transform(apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_numeric_feature_names = numeric_features\n",
    "print(transformed_numeric_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_prepared #So now apple_prepared is matrices !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_prepared.shape #And the shape is what it should so let's train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Test of several models\n",
    "classifiers = {\n",
    "    'Logistic Regression' : LogisticRegression(penalty='l2', solver='lbfgs', C=1.0, max_iter=1000),\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'LightGBM': LGBMClassifier(),\n",
    "    'CatBoost': CatBoostClassifier()\n",
    "}\n",
    "\n",
    "# I'll do cross validation\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='precision')\n",
    "    recall_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='recall')\n",
    "    f1_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='f1')\n",
    "    \n",
    "    print(f\"{name}: Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"   Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std():.4f})\")\n",
    "    print(f\"   Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std():.4f})\")\n",
    "    print(f\"   F1-score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to chose CatBoost and Random Forest to fit the model and then I'll be able to see which one is really the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Let's search parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(apple, apple_label)\n",
    "\n",
    "print(\"Best params : \", grid_search.best_params_)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = best_rf\n",
    "scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='accuracy')\n",
    "precision_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='precision')\n",
    "recall_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='recall')\n",
    "f1_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std():.4f})\")\n",
    "print(f\"Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std():.4f})\")\n",
    "print(f\"F1-score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),                \n",
    "    'max_depth': [None] + list(range(1, 51, 5)),        \n",
    "    'min_samples_split': randint(2, 20),                \n",
    "    'min_samples_leaf': randint(1, 20)                 \n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', verbose=2, n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search.fit(apple, apple_label)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres : \", random_search.best_params_)\n",
    "\n",
    "best_rf = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = best_rf\n",
    "scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='accuracy')\n",
    "precision_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='precision')\n",
    "recall_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='recall')\n",
    "f1_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std():.4f})\")\n",
    "print(f\"Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std():.4f})\")\n",
    "print(f\"F1-score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300],       \n",
    "    'learning_rate': [0.01, 0.05, 0.1],  \n",
    "    'depth': [6, 8, 10],                  \n",
    "    'l2_leaf_reg': [1, 3, 5]              \n",
    "}\n",
    "\n",
    "\n",
    "catboost = CatBoostClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=catboost, param_grid=param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "grid_search.fit(apple, apple_label)\n",
    "\n",
    "print(\"Best params : \", grid_search.best_params_)\n",
    "\n",
    "best_catboost = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(depth=8, iterations=300, l2_leaf_reg=5, learning_rate=0.1)\n",
    "scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='accuracy')\n",
    "precision_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='precision')\n",
    "recall_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='recall')\n",
    "f1_scores = cross_val_score(clf, apple, apple_label, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "print(f\"Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std():.4f})\")\n",
    "print(f\"Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std():.4f})\")\n",
    "print(f\"F1-score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll keep this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_mapping = {'good': 1, 'bad': 0}\n",
    "\n",
    "test_set['Quality'] = test_set['Quality'].map(quality_mapping)\n",
    "\n",
    "apple_test_label = test_set[\"Quality\"].copy()\n",
    "\n",
    "apple_test = test_set.drop([\"Quality\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "final_model = best_catboost\n",
    "\n",
    "apple_test_prepared = preprocessor.fit_transform(apple_test)\n",
    "\n",
    "final_predictions = final_model.predict(apple_test_prepared)\n",
    "\n",
    "accuracy = accuracy_score(apple_test_label, final_predictions)\n",
    "precision = precision_score(apple_test_label, final_predictions)\n",
    "recall = recall_score(apple_test_label, final_predictions)\n",
    "f1 = f1_score(apple_test_label, final_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save model\n",
    "with open('Apple_quality.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "\n",
    "# Load model\n",
    "with open('Apple_quality.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to put more doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
